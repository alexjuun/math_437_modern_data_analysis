---
title: "Multiple Linear Regression: Worked Examples"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The Data

We are going to be working with baseball salary data again. This time we will be using data from the Lahman database, found in the `Lahman` package, and hopefully there will be a few players you have heard of.

We're going to try to predict hitters' salary again. This is going to take a bit of setup; this code comes directly from the help files from the Lahman package.

```{r create hitters data, warning = FALSE, message = FALSE}
library(Lahman) # Still baseball data, but more accurate and more current
library(dplyr)

salaries <- Salaries |>
  dplyr::select(playerID, yearID, teamID, salary)
peopleInfo <- People |>
  dplyr::select(playerID, birthYear, birthMonth, nameLast,
         nameFirst, bats)
batting <- battingStats() |> 
  left_join(salaries, 
            by =c("playerID", "yearID", "teamID")) |>
  left_join(peopleInfo, by = "playerID") |>
  mutate(age = yearID - birthYear - 
           1L * (birthMonth >= 10)) |>
  arrange(playerID, yearID, stint)

```

Now let's filter so we just have the 2016 data (the latest year salary information is available) and put in a few extra conditions so that we're pretty sure we only have full-time hitters, not part-time players, injured players, or pitchers.

```{r batting-2016}
batting_2016 <- batting |> filter(yearID == 2016,
                                   !is.na(salary),
                                   G >= 100, AB >= 200
                                   ) |>
  mutate(salary = salary/1000) # salary in thousands
```

## Our First Model

Let's try to predict the salary based on the number of home runs and the number of walks.

```{r model-1}
lm1 <- lm(salary ~ HR + BB, data = batting_2016)
summary(lm1)
# Since we have HR and BB it is multiple regression model. 

```

1. How do we interpret each number in the Estimate column?
# General Form: y=b_0 + b_1x_1+ b_2x_2 
# salary = 885.42 + 121.92(HR) + 70.85
t-test1 = H_0 = B_0 = 0
          H_a : B_0 != 0 (Almost always useless!!!)

t- test in HR row:
H_0: Mean_salary = B_0 + B_2(BB)
H_a: Mean_salary = B_0 + B_1(HR) + B_2(BB)

t- test in BB row:
H_0: Mean_salary = B_0 + B_1(HR)
H_a: Mean_salary = B_0 + B_1(HR) + B_2(BB)

For now using multiple R-squared value. 



### The ANOVA Test

Anova for Multiple Linear Regression
H_0: Mean_y = B_0
H_a: Mean_y = B_0 + B_1(HR) + B_2(BB)
We can do an ANOVA test for the significance of the full model:

```{r ANOVA-1}
F.stat <- summary(lm1)$fstatistic  # value, numdf, dendf: F(numdf, dendf) = value
F.stat
```

```{r ANOVA-2}
# wrapping the assignment in () will also output to console
(p.value <- pf(F.stat[1], F.stat[2], F.stat[3], lower.tail = FALSE))
```

Notice that the p-value we got with the `pf` function is the same as what came out of the `summary` function!

We can also do partial ANOVA tests. R actually will run a partial ANOVA test for the significance of adding each variable to the model one-at-a-time:

```{r ANOVA-3}
anova(lm1)
summary(lm1)
# We are seeing that similar p value of hr in Anova and summary is totally different whereas BB has similiar P value. The order is matter.
```

Notice that the p-value for `BB` is the same as was output in the summary, but the p-value for `HR` is different. It turns out that partial ANOVA is very sensitive to the order that you input the terms into the model:

```{r ANOVA-4}
lm2 <- lm(salary ~ BB + HR, data = batting_2016)
anova(lm2)
summary(lm1)
```

1. Why is this?

### R-Squared

```{r r-squared}
summary(lm1)$r.squared
```

1. How do we interpret the R-squared value of 0.128?

R squared represents the proportion of variation in y can be explained by this model.
=>  12.8% of variation in salary can be explained by differences in HR and BB.
There is affect in salary with these two variables, but there are lots of other variables affect the salaries.

Notice that this is different from:

```{r adj r-squared}
summary(lm1)$adj.r.squared
```

We will talk about adjusted R-squared later in the semester.

### Parameter Confidence Intervals

Confidence intervals for each parameter in the model use the `confint` function:

```{r confint}
confint(lm1) 
```

If you want the confidence interval for the slope corresponding to a specific predictor, you can either specify the row in the summary output or the name of the predictor:

```{r confint-2}
confint(lm1, parm = "HR")
confint(lm1, parm = 2)
```

Notice that we get 95% CI by default. If you want a different confidence level, include the `level` argument:
# We estimate with 95% confidence that(Prefer to use to make sure it is a guess), When BB is held constant, and HR increases by 1 home run, for every additional homerun. The population mean salary increases by somewhere between $30K and $214K. 
1. our CI is a guess
2. Everything else in the model is constant
3. x_j increases by 1
4. what happens to Mean_y

```{r confint-3}
confint(lm1, parm = 2, level = 0.90)
```

### Prediction

To predict the salary for new players, we use the `predict` function.

```{r pred-1}
predict(lm1) |> head(10) # first 10 predictions
```

When we don't pass in a `newdata` argument, it just predicts for each row in the training set. We should typically pass in the holdout set for `newdata`. Here we don't have a holdout set, so I just create some fake data:

```{r pred-2}
new.df <- tibble(HR = c(2, 10, 15), BB = c(30, 50, 60))

predict(lm1, newdata = new.df)
```

It's much easier to read out if we add the predictions to the data frame. The `broom` package makes this really easy:

```{r pred with broom}
broom::augment(lm1,
               newdata = new.df)
#fitted is y hat
```

We can get confidence intervals for $\mu_{Y}$ and prediction intervals for $y$ by adding additional arguments to the `predict` function.

```{r confint and predint}
predict(lm1, newdata = new.df, interval = "confidence", level = 0.95)
predict(lm1, newdata = new.df, interval = "prediction", level = 0.95)
```

1. What's the difference between these intervals?

We can also use `augment` to make our life much easier:

```{r pred with broom CI}
broom::augment(lm1,
               newdata = new.df,
               interval = "confidence")
```
"confidence": Get us a CI for the Population mean salary over all players with (hypothetically) that combination of HR and BB.
"Prediction": Get us a CI for the salary of an individual player with that combo of HR and BB.
2. How do we interpret the 95% confidence interval in row 1?

## Checking Model Assumptions

The workhorse plot is our residual vs. fit plot:
R
```{r residual plot}
plot(lm1, which = 1)
```


1. What patterns do you notice in this residual plot?
(a)Residuals are not related to fitted yhat. 
- no trend in residuals
- mean 0 at all yhat
- variation in residuals does not depend on y hat (homoskedasticity)
(b)Residuals are normally distributed
(C)Residuals are uncorrelated with each other!


To check normality we can create a normal q-q plot. The `qqnorm` plot will do this with any dataset, but if we want specifically a q-q plot of the residuals, we can use:

```{r qqplot}
plot(lm1, which = 2)
# Z score of residuals on y-axis. Z score of residuals on x-axis. For x convert to percentile, and then convert to z score.-1 to 1 is accurate. Concerning and it's not normally distributed.
```
# If it is perfectly normal distribution, Q-Q residuals must be perfect with the line.
#qnorm(0.05)= Theoretical Quantiles z scores which is x axis
#Y is z score of actual data

2. What does this plot tell us? Does this confirm any suspicions from the residual plot?
Worried about skewed. tail probability issue. range out of [-1,1] it's way off from line.

If we're unsure about heteroskedasticity, we can use the scale-location plot to help us:
# If red line is straight, it means good.

```{r scale-location}
plot(lm1, which = 3)
```
# MUST!!! check residuals and QQ plot which=1 and which=2
Three other plots help us identify potential outliers and high-leverage points:
#Cook's Distance
#High Leverage Points can mess fitted model. #high leverage meaning how much that data affects to linear regression.

```{r cook and leverage}
plot(lm1, which = 4)  # Cook's distance
plot(lm1, which = 5)  # residual vs. leverage
plot(lm1, which = 6)  # Cook's distance vs leverage
```
# Note: Outlier can be:
outliers in response variable
outliers in exploratory variable
outliers that don't fit trend

3. Which players get consistently identified as potential outliers? First find the row numbers in the plots above, then fill in the code chunk below to identify the players corresponding to those row numbers:

```{r check outliers}
batting_2016 |> 
  slice(26,29,134) |> # fill in slice() with the identified row numbers
  select(nameFirst, nameLast, HR, BB, salary, age)
```

Are there any players with crazy-high leverage?

```{r check leverage-1}
hist(hatvalues(lm1))
```

We expect the leverage to be right-skewed, but I don't see any outliers, so probably not anything super-concerning. The highest leverage player is...

```{r check leverage-2}
batting_2016 |>
  slice(which.max(hatvalues(lm1))) |>
  select(nameFirst, nameLast, HR, BB, salary, age)
```

# Note :The x variables in the model are not correlated!
Collinearity : two x-vars highly correlated
Variance inflation factor(VIF): Var(b_j|full model) / var(b_j | model witho only x_j)
Multicollinearity: a linear combo of one set of x-vars is highly correlated with a linear combination of a different set of x-vars



## Collinearity and Multicollinearity

To check for collinearity, we can obtain the correlation matrix:

```{r check collinearity}
with(batting_2016, cor(HR, BB))
```

The correlation here is about 0.5. Is this a big deal? Hard to tell. We can check the variance inflation factor (vif):

```{r vif}
library(car)
vif(lm1)
```

A vif of 1.35 is not that bad. This suggests that even though we have some collinearity, it will not massively affect our coefficient estimates or standard errors.

When we have more than 2 predictors in the model, we have to also watch out for multicollinearity. For an example of extreme collinearity and multicollinearity, we will use three predictors: slugging percentage, on-base percentage, and OPS (the sum of on-base percentage and slugging percentage): 

```{r correlation matrix}
cor(batting_2016 |> select(SlugPct, OBP, OPS))
```

Notice that any model with two of these predictors could be massively affected by collinearity:

```{r collinearity}
lm3 <- lm(salary ~ SlugPct + OPS, data = batting_2016)
summary(lm3)

vif(lm3)
```

Yep, a variance inflation factor of 10.6 suggests major collinearity issues! (By convention, if all vifs are below 5, we're probably okay to assume the variables are uncorrelated, if any vif is above 10, that's a serious problem, and if the highest vif is between 5 and 10 that's a judgment call.)

But look at what happens when we try to include all three predictors:

```{r multicollinearity}
lm.simple <- lm(salary ~ OBP + SlugPct + OPS, data = batting_2016)
summary(lm4) # Can't even fit the model!
```

The `vif` function throws an error that sheds some light on the issue:

```{r vif with multicollinearity, eval = F}
vif(lm4)
```
#"Aliased coefficients" High VIF is bad.

1. What is going on here? Why can't we even fit this model?


## Indicator Variables in Multiple Linear Regression

Let's add a single indicator variable for the league the player is in:

```{r dummy-1}
lm.dummy <- lm(salary ~ lgID, data = batting_2016)
summary(lm.dummy)
```
lgIDNL = 1 if lgID == "NL" 
         0 if not
         
1. What is our reference level for `lgID` here?
"AL"

2. What is our least-squares regression equation?
salary = 7364.8 + (-2071.1)(lgID == "NL")
plugging in 1 if NL plugging in 0 when AL

3. What is the predicted salary for a player in the American League? In the National League? 
Just intercept for AL= 7364.8
NL = 7364.8 + (-2071.1)(1) = 

4. How do we interpret the slope in this model?
A player in NL is predicted/expected to make $2,071,100 less than a player in AL.

By default, the reference level is the first alphabetically. To use a different reference level, we need to use the `relevel` function. However, this function only works on factor variables:

```{r relevel}
batting_2016a <- batting_2016 |> mutate(league = as.factor(lgID))
batting_2016a$league <- relevel(batting_2016a$league, ref = "NL")

lm.dummy2 <- lm(salary ~ league, data = batting_2016a)
summary(lm.dummy2)
```

Notice what happens with the coefficients:

```{r dummy coefficients}
coef(lm.dummy)
coef(lm.dummy2)
```

### Multiple Indicator Variables

Let's subset to just the 5 NL West teams:

```{r multiple indicator variables}
nlwest <- batting_2016 |> filter(teamID %in% c("LAN", "SFN", "SDN", "COL", "ARI"))
lm.dummy3 <- lm(salary ~ teamID, data = nlwest)
summary(lm.dummy3)
```

1. What is our reference level here?
Arizona

2. What is our least-squares regression equation?
salary= 2713 + 2479(TeamID == "COL") + 3309(teamID == "LAN") + 1134(teamID == "SDN") + 6333(teamID == "SFN)

3. How do we interpret the slope corresponding to `LAN` in this model?
A player on LA is predicted/expected to make $3.3 millions more than a player on Arizona.

```{r relevel Eval=False}
nlwesta <- nlwest |> mutate(team = as.factor(lgID))
batting_2016a$league <- relevel(batting_2016a$league, ref = "NL")

lm.dummy2 <- lm(salary ~ league, data = batting_2016a)
summary(lm.dummy2)
```

Notice that the F-statistic and p-value for this model also test $H_0:$ the population mean salary is the same for all 5 teams. Compare to the one-way ANOVA:

```{r lm vs anova}
# One-way ANOVA
oneway.test(salary ~ teamID, data = nlwest, var.equal = TRUE)
```
If anova, it's possible to write it as linear.

## Interaction Effects
#Note: Interaction effects: The relationship between y and x_1 depends on the value of x_2!!

An interaction effect is coded with a `:`

```{r interaction-1}
lm.interaction <- lm(salary ~ HR + BB + HR:BB, data = nlwest) #Interaction between HR and BB represented as HR:BB 
summary(lm.interaction)
```

1. What is our least-squares regression equation?
salary = -4405.806 + 644.147(HR) + 219.8876(BB) + (-12.672)(HR)(BB)

2. How do we interpret the slope corresponding to `HR` in this model?
For every additional HR, the predicted salary increases by 644,147, holding BB constant at 0!!!

3. What does it mean for the interaction term to have a negative slope?


Notice that all three slopes are significant at the 5% significance level, but the overall model is not! Once we start adding interaction effects, hypothesis tests start getting a bit wonky due to collinearity issues:

```{r collinearity interaction}
cor(model.matrix(lm.interaction)[,-1]) # 1st column is intercept
```

We know that adding another term to the model will make the R-squared increase. 

```{r r-squared comparison}
lm.no_interaction <- lm(salary ~ HR + BB, data = nlwest)

summary(lm.no_interaction)$r.squared
summary(lm.interaction)$r.squared
```

Is this a significant increase?

```{r anova for interaction term}
anova(lm.interaction)
```

### Including Main and Interaction Effects

#Note: Hierarchical Principle: If we include an interaction effect in the model, we must include the main effect of all variables in the interaction.

`*` is a shorcut for combining `+` and `:`, therefore to include both main and interaction effects of HR and BB:
if a,b,c variables(columns)
a*b*c = a + b + c + a:b + a:c + b:c + a:b:c + ab:c + a:bc + ac:b

```{r main and interaction}
lm.equivalent <- lm(salary ~ HR * BB, data = nlwest)

summary(lm.interaction)
summary(lm.equivalent)
```

### Looking for Interaction Effects

Remember that we can look at the combined effect of a quantitative and categorical predictor on the quantitative response by adding colors on a scatterplot:

```{r interaction plot 1, warning = FALSE, message = FALSE}
library(ggplot2)
interact_plot <- ggplot(batting_2016a, aes(x = HR, y = salary)) +
  geom_point(aes(color = league)) +  # color-code points
  geom_smooth(aes(color = league), method = "lm", se = FALSE)  # add a regression line for each group
print(interact_plot)
```

1. Does there appear to be an interaction effect between `league` and `HR`? Why or why not?

Here's the linear regression model:

```{r interaction model 2}
lm.interaction2a <- lm(salary ~ HR * league, data = batting_2016a)
summary(lm.interaction2a)
```

2. How do we interpret the slopes corresponding to each main effect?

3. What would be the equation for predicting salary from HR looking only at the AL? Looking only at the NL?

We can change the reference level:

```{r change ref level}
batting_2016a$league <- relevel(as.factor(batting_2016a$league), ref = "AL")

lm.interaction2b <- lm(salary ~ HR * league, data = batting_2016a)
summary(lm.interaction2b)
```

4. According to this summary, what would be the equation for predicting salary from HR looking only at the AL? Looking only at the NL?

In base R, we can use `interaction.plot`, but I think that the scatterplot is much easier to read:

```{r interaction.plot}
batting_cleaned <- batting_2016a |> 
  mutate(league = factor(league, levels = c("AL", "NL")))
with(batting_cleaned, interaction.plot(x.factor = HR, trace.factor = league, response = salary))
```